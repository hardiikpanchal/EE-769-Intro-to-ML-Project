# -*- coding: utf-8 -*-
"""IML_Project_DCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E1YqCeB2HJQNT1jrm5SkkYI10l-8dDCQ

# DCGAN Implementation
In this project, the research paper "UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS" will be implemented.\
Link to the paper: https://arxiv.org/pdf/1511.06434.pdf

# Abstract of paper
In order to bridge the gap between supervised and unsupervised CNNs, the author of the paper came up with a new class of networks - deep convolutional generative adversarial networks aka DCGANs. Throughout the paper, the model has been trained on various datasets to show that it performs well on various representations (such as objects as well as scenery) in both the generator and dicriminator parts of the model.

# The following will be achieved throughout the course of this paper
1.   Evaluate a set of constraints on topology of DCGANs that allows them to be stable while training on data.
2.   Convince good performance of trained dicriminator models for image classification.
3.   Visualize filters learnt by DCGANs.
4. Show that the generators have interesting vector arithmetic properties allowing for easy
manipulation of info in generated images/samples

# Details of training on the dataset
- Trained on LSUN, Imagenet-1k and Faces dataset
- Scaling of images to the range of tanh activation function [-1,1] 
- Trained with Mini-Batch Stochastic Gradient Descent with mini-batch size = 128
- Weights were initialized from Normal distribution with standard deviation 0.02 and mean = 0 
- Slope of Leaky ReLU 0.2
- Adam optimizer is used for training
- Learning rate = 0.0002 for best results
- Momentum term Beta1 = 0.5 instead of 0.9 to avoid oscillations & instability

# Information on datasets

## LSUN
The dataset can be found here: https://paperswithcode.com/dataset/lsun
- Contains 3 million samples of high resolution bedrooms.
- No data augmentation applied to the data
- Used to demonstrate how well the model handles more and high resolution data
- De-duplication done to prevent overfitting (removed 275k samples)

## FACES
The dataset can be found here: https://www.kaggle.com/datasets/ashwingupta3012/human-faces
- This dataset contains 3 million images of over 10000 people
-  No data augmentation was applied to the images
- OpenCV face detection model was used on the dataset to generate about 350k face boxes which were used for training

## Imagenet-1k
The dataset can be found here: https://www.image-net.org/download.php
- This dataset contains natural images for unspuervised learning
- We train on images of size 64 X 64
- No data augmentation was applied to the images

## CIFAR-10
The dataset can be found here: https://www.cs.toronto.edu/~kriz/cifar.html
- We will be performing classification of CIFAR-10 dataset using GANs as feature extractor.
-The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. 
-There are 50000 training images and 10000 test images. 
-Here are the classes in the dataset: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck

##SVHN
The SVHN dataset can be found here: https://www.kaggle.com/datasets/hugovallejo/street-view-house-numbers-svhn-dataset-numpy

- There are 73257 digits for training and 26032 digits for testing
- All images are 32X32 in size
- There are 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 10.

#Importing Libraries and Modules
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'
import matplotlib.pyplot as plt
from os.path import exists
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from google.colab import files

from __future__ import print_function
import os
import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils

from google.colab import drive
drive.mount('/content/gdrive')

!pip install gdown

"""# Defining DCGAN architecture"""

cudnn.benchmark = True
#set seed to constant
seed = random.randint(1, 10000)
random.seed(seed)
torch.manual_seed(seed)
#check the availability of cuda devices
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# number of gpu's available
ngpu = 1

if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

# random weights initialization of netG and netD as given in paper
#Gaussian distribution with S.D. = 0.02
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)

#generator class for DCGAN architecture
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(64 * 8),
            nn.ReLU(True),
            # state size. (64*8) x 4 x 4
            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64 * 4),
            nn.ReLU(True),
            # state size. 64*4 x 8 x 8
            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64 * 2),
            nn.ReLU(True),
            # state size. 64*2 x 16 x 16
            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            # state size. (64) x 32 x 32
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (3) x 64 x 64
        )
        
    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)
            return output
#discriminator class for DCGAN architecture
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (3) x 64 x 64
            nn.Conv2d(3, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64) x 32 x 32
            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64 * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64*2) x 16 x 16
            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64 * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64*4) x 8 x 8
            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64 * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (64*8) x 4 x 4
            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

#fucntion for forward pass on model
    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)

        return output.view(-1, 1).squeeze(1)

def trainDCGAN(dataloader,dataset,max_iterations,chkpt=5):
  
  #create generator instance with DCGAN architecture
  netG = Generator(ngpu).to(device)
  #load initialized weights  
  netG.apply(weights_init)
  print(netG)
 
  #create discriminator instance with DCGAN architecture
  netD = Discriminator(ngpu).to(device)
  #load initialized weights  
  netD.apply(weights_init)
  print(netD)

  #cross entropy loss for training the model
  criterion = nn.BCELoss()

  # setup optimizer
  optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))
  optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))

  #fixed noise vector for comparison between generated images 
  fixed_noise = torch.randn(128, 100, 1, 1, device=device)
  real_label = 1
  fake_label = 0

  niter = max_iterations

  for epoch in range(niter):
      for i, data in enumerate(dataloader, 0):
          #Update Discriminator
          # train with real data 
          netD.zero_grad()
          #transfer data to device where model is loaded (GPU/CPU) for training
          data = data[0].to(device)
          batch_size = data.size(0)
          #set label of all images from dataset as real 
          label = torch.full((batch_size,), real_label, device=device)
          
          #calculate loss for Discriminator on current batch data
          output = netD(data)
          errD_real = criterion(output.float(), label.float())
          #backpropagation
          errD_real.backward()
          D_x = output.mean().item()

          # train with fake data generated by Generator 
          noise = torch.randn(batch_size, 100, 1, 1, device=device)
          #generate fake image using random noise vector
          fake = netG(noise)
          #set label of all images as fake 
          label.fill_(fake_label)
          #detach because gradient isn't to be computed 
          output = netD(fake.detach())
          #calculate error on fake data
          errD_fake = criterion(output.float(), label.float())
          errD_fake.backward()
          D_G_z1 = output.mean().item()
          #total error = error on real data+fake(generated data)
          errD = errD_real + errD_fake
          #update weights
          optimizerD.step()

         #Update Generator
          netG.zero_grad()
          label.fill_(real_label)  # fake labels are real for generator cost
          output = netD(fake)
          errG = criterion(output.float(), label.float())
          #backward propagation
          errG.backward()
          D_G_z2 = output.mean().item()
          #update generator weights
          optimizerG.step()
          #display losses 
          print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f' % (epoch, niter, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))
          
          #save the output 
          g_loss.append(errG.item())
          d_loss.append(errD.item())
          #save real and fake images generated by DCGAN once each epoch
          if i % 50 == 0:
              print("Saving generated output image")
              vutils.save_image(data,f'/content/gdrive/MyDrive/images/{dataset}/real_sample.png',normalize=True)
              fake = netG(fixed_noise)
              img = fake.detach().cpu()
              #append generated fake image to images list
               
              images.append(np.transpose(vutils.make_grid(img, padding=2, normalize=True), (1,2,0)))
              vutils.save_image(img,f'/content/gdrive/MyDrive/images/{dataset}/fake_samples_epoch_{epoch+1}.png',normalize=True)
      
      # save model checkpoints once in chkpt epochs
      if((epoch+1)%chkpt==0):
        gpath = f"{dataset}_netG_epoch{epoch+1}.pth"
        dpath = f"{dataset}_netD_epoch{epoch+1}.pth"
        dest = f"/content/gdrive/MyDrive/models/{dataset}"

        #save generator model
        torch.save(netG.state_dict(), gpath)
        !cp -r {gpath} {dest}
        #save discriminator model
        torch.save(netD.state_dict(), dpath)
        !cp -r {dpath} {dest}

#helper function to display images 
def imshow(img):
    image = np.transpose(img[0].cpu().detach().numpy()[0], (1, 2, 0))
    #convert to numpy from tensor
    print(image.shape)
    plt.imshow(image)

#resize image to 64*64 and convert to tensor
#normalize pixel values to make their range = [-1,1] = range of tanh function
img_transform = transforms.Compose([
                               transforms.Resize((64,64)),
                               
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
                           ])

"""#Training DCGAN on ImageNet-1k dataset

##Loading ImageNet-1k dataset
"""

!pip install -q kaggle
from google.colab import files
files.upload()
#json file has to be uploaded for downloading from kaggle

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d akash2sharma/tiny-imagenet
#download dataset

!mkdir train
!unzip tiny-imagenet.zip -d train
#unzip file

#copy dataset into gdrive 
!cp -r '/content/train/tiny-imagenet-200/val' '/content/gdrive/MyDrive/data/'

dataset = "ImageNet1k"
!mkdir {dataset}
#make folder to save output images of model trained on ImageNet1k

"""## Data Pre-Processing """

#create pytorch dataloader for training from ImageNet dataset
train_dataset = datasets.ImageFolder('/content/train/tiny-imagenet-200/val', transform=img_transform)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)

print(train_dataloader.dataset)

imshow(next(iter(train_dataloader)))
#diplaying first image from dataset

"""##Training"""

#initializing losses and image lists for GAN 
g_loss = []
d_loss = []
images =[]

trainDCGAN(train_dataloader,"ImageNet1k",40,5)

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(g_loss,label="G")
plt.plot(d_loss,label="D")
plt.xlabel("No. of iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""## Results"""

#load generator model checkpoint
genmodel = Generator(ngpu).to(device)
genmodel.load_state_dict(torch.load('ImageNet1k_netG_epoch40.pth'))

#load discriminator model checkpoint
Dmodel = Discriminator(ngpu).to(device)
Dmodel.load_state_dict(torch.load('ImageNet1k_netD_epoch40.pth'))

for i in range(3):
  # Turn off gradient calculation to speed up processing
    with torch.no_grad():
    # Get generated image from the noise vector using the trained generator 
      noise = torch.randn(1, 100, 1, 1, device=device)
      generated_img = genmodel(noise)
      print(Dmodel(generated_img))
      img = generated_img.detach().cpu()
      
    # Display the generated image
    plt.axis("off")
    plt.title("Generated Image")
    plt.imshow(np.transpose(vutils.make_grid(img, padding=2, normalize=True), (1,2,0)))
    plt.show()

real_img, label =  next(iter(train_dataloader))
print(real_img.shape)
print(Dmodel(real_img.to(device)))

images[0].size() #gride of fake images generated in different epochs by Generator while training

# Grab a batch of real images from the dataloader
real_batch = next(iter(train_dataloader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(images[-1][529:])
plt.show()

animate_images=[]
for i in range(len(images)):
  if((i+1)%3==0):
    animate_images.append(images[i])

import matplotlib
import matplotlib.animation as animation
from IPython.display import HTML
fig = plt.figure(figsize=(8,8))
plt.axis("off")
ims = [[plt.imshow(i, animated=True)] for i in animate_images]
#animating images generated by model at different stages of training
ani = animation.ArtistAnimation(fig, ims, interval=500, repeat_delay=500, blit=True)

HTML(ani.to_jshtml())

ani.save('/content/gdrive/MyDrive/DCGANTraining.gif')

"""#Training DCGAN on Human Faces dataset

## Loading Faces dataset
"""

!gdown --fuzzy https://drive.google.com/file/d/1C8LfmS2tdOAHWAptn9cDSyFelYWizNIy/view?usp=sharing 
#download faces dataset

!unzip /content/faces_zip.zip -d faces

"""##Data Pre-Processing

"""

#create pytorch dataloader for training 
train_faces_dataset = datasets.ImageFolder('/content/faces/', transform=img_transform)
train_faces_dataloader = torch.utils.data.DataLoader(train_faces_dataset, batch_size=128, shuffle=True)

next(iter(train_faces_dataloader))[0].numpy().shape

print(train_faces_dataloader.dataset)

imshow(next(iter(train_faces_dataloader)))

dataset = "Faces"
!mkdir {dataset}

"""##Training """

#initializing losses and image lists for GAN 
g_loss = []
d_loss = []
images =[]

trainDCGAN(train_faces_dataloader,"Faces",15,3)

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training ")
plt.plot(g_loss,label="G")
plt.plot(d_loss,label="D")
plt.xlabel("No. of iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""## Results"""

#load generator model checkpoint (most recent one)
genmodel = Generator(ngpu).to(device)
genmodel.load_state_dict(torch.load('Faces_netG_epoch15.pth'))
#load generator model checkpoint (most recent one)
Dmodel = Discriminator(ngpu).to(device)
Dmodel.load_state_dict(torch.load('Faces_netD_epoch15.pth'))

# Turn off gradient calculation
for i in range(3):
    with torch.no_grad():
    # Get generated image from the noise vector using trained generator
    
      noise = torch.randn(1, 100, 1, 1, device=device)
      generated_img = genmodel(noise)
      print(Dmodel(generated_img))
      img = generated_img.detach().cpu()
      
    # Display the generated image.
    plt.axis("off")
    plt.title("Generated Image")
    plt.imshow(np.transpose(vutils.make_grid(img, padding=2, normalize=True), (1,2,0)))

    plt.show()

# Grab a batch of real images from the dataloader
real_batch = next(iter(train_faces_dataloader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(images[-1][0:529])
plt.show()

"""#Training DCGAN on LSUN dataset

## Loading LSUN dataset
"""

!pip install -q kaggle
files.upload()
#json file has to be uploaded for downloading from kaggle

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ajaykgp12/lsunchurch
#download Large-scale Scene Understanding (LSUN) dataset

!mkdir train
!unzip /content/lsun_zip.zip -d lsun
#unzip file

# !gdown --fuzzy https://drive.google.com/file/d/1m7GF_2AEhblE6wm7VZrKgvYPKvHZP7mT/view?usp=share_link
# #download Large-scale Scene Understanding (LSUN) dataset

"""##Data Pre-Processing

"""

import numpy as np
lsun_data = np.load('/content/lsun/church_outdoor_train_lmdb_color_64.npy')

lsun_data.shape

lsun_data = lsun_data[:50000]
#take only first 10000 images

lsun_data.shape

lsun_data = (lsun_data - 127.5) / 127.5  # Normalize the images to [-1, 1]

lsun_data = np.transpose(lsun_data,(0,3,1,2)) #switch dimensions to batchsize,channels,height,width
lsun_data = lsun_data[:,None,:,:,:]

lsun_data.shape

from torch.utils.data import TensorDataset

train_LSUN_dataset = torch.Tensor(lsun_data) # transform to torch tensor

#create pytorch dataloader for training 
train_LSUN_dataloader = torch.utils.data.DataLoader(train_LSUN_dataset, batch_size=128, shuffle=True)

print(train_LSUN_dataloader.dataset[0])

next(iter(train_LSUN_dataloader)).shape

imshow(next(iter(train_LSUN_dataloader)))

type(next(iter(train_LSUN_dataloader)))

#helper function to display images 
def imshow_lsun(img):
    #convert to numpy from tensor
    #image = img[0].cpu().detach().numpy() 
    image = np.transpose(img[0].cpu().detach().numpy()[0], (1, 2, 0)) 
    print(image.shape)
    plt.imshow(image)
imshow_lsun(next(iter(train_LSUN_dataloader)))

dataset = "LSUN"
!mkdir {dataset}

"""##Training"""

#initializing losses and image lists for GAN 
g_loss = []
d_loss = []
images =[]

trainDCGAN(train_LSUN_dataloader,"LSUN",20,5)

plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training ")
plt.plot(g_loss,label="G")
plt.plot(d_loss,label="D")
plt.xlabel("No. of iterations")

plt.ylabel("Loss")
plt.legend()
plt.show()

"""## Results"""

#load generator model checkpoint (most recent one)
genmodel = Generator(ngpu).to(device)
genmodel.load_state_dict(torch.load('LSUN_netG_epoch20.pth'))
#load discriminator model checkpoint (most recent one)
Dmodel = Discriminator(ngpu).to(device)
Dmodel.load_state_dict(torch.load('LSUN_netD_epoch20.pth'))

# Turn off gradient calculation
for i in range(3):
    with torch.no_grad():
    # Get generated image from the noise vector using trained generator
    
      noise = torch.randn(1, 100, 1, 1, device=device)
      generated_img = genmodel(noise)
      print(Dmodel(generated_img))
      img = generated_img.detach().cpu()
      
    # Display the generated image.
    plt.axis("off")
    plt.title("Generated Image")
    plt.imshow(np.transpose(vutils.make_grid(img, padding=2, normalize=True), (1,2,0)))

    plt.show()

# Grab a batch of real images from the dataloader
real_batch = next(iter(train_LSUN_dataloader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Image")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(images[-1][0:529])
plt.show()

for i in range (5):
  real_img =  next(iter(train_LSUN_dataloader))[i]
  print(Dmodel(real_img.to(device)))

"""# Manipulating Generator Representations"""

#load generator model checkpoint (most recent one)
genmodel = Generator(ngpu).to(device)
genmodel.load_state_dict(torch.load('/content/gdrive/MyDrive/models/Faces_netG_epoch15.pth'))

import tensorflow as tf
faces = []

def generateCompositions(seed):
  n = 5 
  torch.manual_seed(seed)
  #fix two noise vectors
  Z1 = torch.randn(1, 100, 1, 1, device=device)
  Z2 = torch.randn(1, 100, 1, 1, device=device)

  face1 =[]
  face2 = []
  face = []
  for j in range (n):
    with torch.no_grad():
      # Get generated image from the noise vector using trained generator
        generated_img1 = genmodel(Z1)
        generated_img2 = genmodel(Z2)
        Z = Z1+Z2
        #sum of given noise vectors
        generated_img = genmodel(Z1+Z2)

        #generate images for all noise vectors
        img1 = generated_img1.detach().cpu()
        img2 = generated_img2.detach().cpu()
        img = generated_img.detach().cpu()

        face1.append(img1)
        face2.append(img2)
        face.append(img)
  #average out the faces generated by model
  avgface1 = (face1[0])/n
  avgface2 = (face2[0])/n
  avgface = (face[0])/n
  for k in range(n-1):
    avgface1 += (face1[k]/n)
    avgface2 += (face2[k])/n
    avgface += face[k]/n
#append final faces: face1, face2, face generated by Z1+Z2, face obtained by averaging pixels fo face1 and face2
  faces.append(avgface1)
  faces.append(avgface2)
  faces.append(avgface)
  faces.append((avgface1+avgface2)/2)

for i in range(3):
    generateCompositions(i*3)
#generate faces to display

faces = [np.transpose(vutils.make_grid(face, padding=1, normalize=True), (1,2,0)) for face in faces]

imgs = faces
_, axs = plt.subplots(3, 4, figsize=(8, 8))
axs = axs.flatten()
#display faces in a grid
for img, ax in zip(imgs, axs):
    ax.imshow(img)
plt.show()

"""# Empirical Validation of DCGANs capabilities

## Classifying CIFAR-10 using GANs as feature extractor

The CIFAR-10 dataset can be found here: https://www.cs.toronto.edu/~kriz/cifar.html
- We will be performing classification of CIFAR-10 dataset using GANs as feature extractor.
- The performance of GANs can be determined by using them as feature extractors and evaluating the features on a linear model.
- We evaluate the quality of representations learned by DCGANs by training them on Imagenet-1k and then use the discriminator’s convolutional features from all layers, maxpooling each layers representation to produce a 4 x 4 spatial grid.
"""

#PyTorch imports
import torchvision.models as models
import torchvision.transforms as transforms
from torch.autograd import Variable
from PIL import Image
import torch

"""###Loading pre-trained ImageNet model"""

# Load the pretrained model
model = Discriminator(ngpu).to(device)
model.load_state_dict(torch.load('/content/gdrive/MyDrive/IML_Project/ImageNet_netD_epoch24.pth', map_location=device))
# loading the discriminator model trained on ImageNet
model.eval() #setting it to eval mode

print([n for n, _ in model.named_modules()]) #checking layer info

"""###Function to get feature matrix"""

def get_vector(path, layer, index): #function to get the feature vector from image which takes layer and image path as arguments
    scaler = transforms.Resize((64, 64))
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    to_tensor = transforms.ToTensor()
    #few transfroms defined to get image in specific shape

    layer = model.main[layer] #selecting layer based on input
    img = Image.open(path)
    # print("shape", img)
    # 2. Create a PyTorch Variable with the transformed image
    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))
    # print(t_img)
    # 3. Create a vector of zeros that will hold our feature vector
    #    The 'avgpool' layer has an output size of 512
    my_embedding = torch.zeros(1, int(512/index), 4*index, 4*index)
    def copy_data(m, i, o):
            # print(o)
            # print(o.shape)
            my_embedding.copy_(o)
        # 5. Attach that function to our selected layer
    h = layer.register_forward_hook(copy_data)
        # 6. Run the model on our transformed image
    model(t_img.to(device))
        # 7. Detach our copy function from the layer
    h.remove()
    return my_embedding
    # 7. Detach our copy function from the layer

def get_feature(path):
    op1 = get_vector(path, 8, 1)
    op2 = get_vector(path, 5, 2)
    op3 = get_vector(path, 2, 4)
    op4 = get_vector(path, 0, 8)

    # print(op1.shape)
    # print(op2.shape)
    # print(op3.shape)
    # print(op4.shape)

    #applying maxpooling on all 4 hidden layer activations 
    pooling = nn.MaxPool2d(4)
    output1 = pooling(op1)
    output2 = pooling(op2)
    output3 = pooling(op3)
    output4 = pooling(op4)
    # print(output1.shape)
    # print(output2.shape)
    # print(output3.shape)
    # print(output4.shape)
    t1 = torch.flatten(output1)
    t2 = torch.flatten(output2)
    t3 = torch.flatten(output3)
    t4 = torch.flatten(output4)
    #conctenating all activations to form final feature vector
    t1 = t1.detach().numpy()
    t2 = t2.detach().numpy()
    t3 = t3.detach().numpy()
    t4 = t4.detach().numpy()
    feature = np.concatenate((t1, t2, t3, t4))
    # feature = feature.tolist() 
    return feature

"""###Importing dataset from Kaggle"""

!pip install -q kaggle #using kaggle to import dataset into colab
from google.colab import files
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d swaroopkml/cifar10-pngs-in-folders #downloading dataset

!mkdir data_cifar10_new 
!unzip cifar10-pngs-in-folders.zip -d data_cifar10_new #preparing data folder

import os #some imports needed
from os import listdir

"""###Getting features for training and test data"""

def get_features(data_type, max_len): #function to get the full feature vector for all train and test images
  feature_matrix = [] # data type can be train or test
  #taking features from all 10 classes
  # get the path/directory
  folder_dir1 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/airplane"
  iter = 0
  for images in os.listdir(folder_dir1):
    iter = iter + 1 #specifying max no of points to be taken
    if(iter > max_len):
      iter = 0
      break
    path_img1 = folder_dir1 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img1))
  
  folder_dir2 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/automobile"

  for images in os.listdir(folder_dir2):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img2 = folder_dir2 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img2))
  
  folder_dir3 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/bird"

  for images in os.listdir(folder_dir3):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img3 = folder_dir3 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img3))
  
  folder_dir4 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/cat"

  for images in os.listdir(folder_dir4):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img4 = folder_dir4 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img4))
  
  folder_dir5 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/deer"

  for images in os.listdir(folder_dir5):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img5 = folder_dir5 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img5))
  
  folder_dir6 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/dog"

  for images in os.listdir(folder_dir6):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img6 = folder_dir6 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img6))
  
  folder_dir7 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/frog"

  for images in os.listdir(folder_dir7):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img7 = folder_dir7 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img7))
  
  folder_dir8 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/horse"

  for images in os.listdir(folder_dir8):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img8 = folder_dir8 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img8))
  
  folder_dir9 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/ship"

  for images in os.listdir(folder_dir9):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img9 = folder_dir9 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img9))
  
  folder_dir10 = "/content/data_cifar10_new/cifar10/cifar10/" + data_type + "/truck"

  for images in os.listdir(folder_dir10):
    iter = iter + 1
    if(iter > max_len):
      iter = 0
      break
    path_img10 = folder_dir10 + '/' + images
    # print(path_img1)
    feature_matrix.append(get_feature(path_img10))
  
  feature_matrix = np.array(feature_matrix)
  return feature_matrix

train_features = get_features("train", 500) #training dataset
test_features = get_features("test", 100) #test dataset

np.savetxt("train.txt", train_features) #saving for fututre purpose
np.savetxt("test.txt", test_features) #saving for fututre purpose
print(train_features.shape)
print(test_features.shape)

train_len = 500 #making label vector for training data
y_train = []
for x in range(train_len):
  y_train.append('airplane')
for x in range(train_len):
  y_train.append('automobile')
for x in range(train_len):
  y_train.append('bird')
for x in range(train_len):
  y_train.append('cat')
for x in range(train_len):
  y_train.append('deer')
for x in range(train_len):
  y_train.append('dog')
for x in range(train_len):
  y_train.append('frog')
for x in range(train_len):
  y_train.append('horse')
for x in range(train_len):
  y_train.append('ship')
for x in range(train_len):
  y_train.append('truck')

print(y_train)

test_len = 100 #making label vector for test data
y_test = []
for x in range(test_len):
  y_test.append('airplane')
for x in range(test_len):
  y_test.append('automobile')
for x in range(test_len):
  y_test.append('bird')
for x in range(test_len):
  y_test.append('cat')
for x in range(test_len):
  y_test.append('deer')
for x in range(test_len):
  y_test.append('dog')
for x in range(test_len):
  y_test.append('frog')
for x in range(test_len):
  y_test.append('horse')
for x in range(test_len):
  y_test.append('ship')
for x in range(test_len):
  y_test.append('truck')

print(y_test)

np.savetxt("trainres.txt", train_features) #saving feature matrix
np.savetxt("testres.txt", test_features) #saving feature matrix

"""###Training RBF Kernel SVM"""

parameters = {'gamma':[0.001, 0.01, 0.1], 'C':[0.1, 1, 10, 100]} #parameters grid for RBF Kernel svm

svc = SVC(kernel="rbf") 

clf_rbfsvm_b = GridSearchCV(svc, parameters, cv = 5, verbose = 3)

clf_rbfsvm_b.fit(train_features, y_train)

clf_rbfsvm_b.best_params_

from sklearn.metrics import classification_report

print(classification_report(clf_rbfsvm_b.predict(test_features), y_test)) #classification report

"""###Training linear SVM"""

parameters = {'C':[0.01, 0.1, 1, 10]} #parameters grid for linear svm

svc = LinearSVC(max_iter = 10000)

clf_linearsvc_b = GridSearchCV(svc, parameters, verbose = 3)

clf_linearsvc_b.fit(train_features, y_train)

clf_linearsvc_b.best_params_

print(classification_report(clf_linearsvc_b.predict(test_features), y_test)) #classification report

"""##Classifying SVHN Dataset using GANs as feature extractor

The SVHN dataset can be found here: https://www.kaggle.com/datasets/hugovallejo/street-view-house-numbers-svhn-dataset-numpy

- There are 73257 digits for training and 26032 digits for testing
- All images are 32X32 in size
- There are 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 10.

### Loading dataset from Kaggle
"""

!kaggle datasets download -d hugovallejo/street-view-house-numbers-svhn-dataset-numpy #downloading dataset

!mkdir data_SVHN
!unzip street-view-house-numbers-svhn-dataset-numpy.zip -d data_SVHN

data = np.load('/content/data_SVHN/X_test.npy') #loading train and test data and label
label = np.load('/content/data_SVHN/y_test.npy')
test = np.load('/content/data_SVHN/X_test.npy')
test_label = np.load('/content/data_SVHN/y_test.npy')

x = np.transpose(data, (3,2,0,1)) #making data into proper shape
print(x.shape)

print(label)

"""###Getting features for training and test data"""

train_SVHN = [] #extracting features for train and test dataset
train_SVHN_y = []
for i in range(10000): #10000 train images
  train_SVHN.append(get_feature2(x[i]))
  train_SVHN_y.append(label[i])
train_SVHN = np.array(train_SVHN)
train_SVHN_y = np.array(train_SVHN_y)
train_SVHN_y = train_SVHN_y.reshape(10000,)

test_SVHN = [] #extracting features for train and test dataset
test_SVHN_y = []
for i in range(20000, 22000): #2000 train images
  test_SVHN.append(get_feature2(x[i]))
  test_SVHN_y.append(label[i])
test_SVHN = np.array(test_SVHN)
test_SVHN_y = np.array(test_SVHN_y)
test_SVHN_y = test_SVHN_y.reshape(2000,)

"""###Training RBF Kernel SVM"""

parameters = {'gamma':[0.001, 0.01], 'C':[1, 10, 100]} #parameters grid for RBF Kernel svm

svc = SVC(kernel="rbf")

clf_rbfsvm_b = GridSearchCV(svc, parameters, cv = 5, verbose = 3)

clf_rbfsvm_b.fit(train_SVHN, train_SVHN_y)

clf_rbfsvm_b.best_params_

print(classification_report(clf_rbfsvm_b.predict(test_SVHN), test_SVHN_y)) #classification report

"""###Training linear SVM"""

parameters = {'C':[0.01, 0.1]} #parameters grid for linear svm

svc = LinearSVC(max_iter = 10000)

clf_linearsvc_b = GridSearchCV(svc, parameters, verbose = 3)

clf_linearsvc_b.fit(train_SVHN, train_SVHN_y)

clf_linearsvc_b.best_params_

print(classification_report(clf_linearsvc_b.predict(test_SVHN), test_SVHN_y)) #classification report

"""# References

- https://www.analyticsvidhya.com/blog/2021/07/deep-convolutional-generative-adversarial-network-dcgan-for-beginners/
- https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/dcgan/dcgan.py
- https://github.com/Newmu/dcgan_code
- https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c
- https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
"""